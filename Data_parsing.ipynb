{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Processing file Australia_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file Britain_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file Canada_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file ICRC_library.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processing file IFRC_Europe.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file Ireland_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processing file Nairobi_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file nepal_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processing file philredcross.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file RedCross.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file RedCrossLebanon.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n"
     ]
    }
   ],
   "source": [
    "all_columns=['conversation_id', 'id','author_id','text','in_reply_to_user_id']\n",
    "for file in os.listdir(\"conversations\"):\n",
    "    if file.split(\".\")[1]==\"json\":\n",
    "        print(\"--------------Processing file {0}--------------\".format(file))\n",
    "        df=pd.read_json(os.path.join(\"conversations\",file),lines=True)\n",
    "        all_tweets=[]\n",
    "        i=100\n",
    "        for line in df['data']:\n",
    "            for tweet in line:\n",
    "                tweet_info=[]\n",
    "                for column in all_columns:\n",
    "                    tweet_info.append(tweet[column])\n",
    "                try:\n",
    "                    if tweet['referenced_tweets'][0]['type']=='replied_to':\n",
    "                        tweet_info.append(tweet['referenced_tweets'][0]['id'])\n",
    "                    else:\n",
    "                        tweet_info.append('')\n",
    "                except:\n",
    "                    print(tweet['referenced_tweets'][0])\n",
    "                    tweet_info.append('')\n",
    "                all_tweets.append(tweet_info)\n",
    "            if len(all_tweets)>i:\n",
    "                print(\"--------------Processed more than {0} tweets--------------\".format(i))\n",
    "                i=i*10\n",
    "        df_out = pd.DataFrame(all_tweets,columns=all_columns+['replied_to'])\n",
    "        df_out.to_csv(\"Parsed_conversations/\"+file.split(\".\")[0]+\".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semester_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9215884f317b28951366236cef9f8992a096a5b1fa6c5fa4d454321ff0331c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
