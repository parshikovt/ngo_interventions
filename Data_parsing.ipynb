{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Processing file Australia_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file Britain_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file Canada_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file ICRC_library.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file IFRC_Europe.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file Ireland_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file Nairobi_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file nepal_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processing file New_Australia_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file New_Britain_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processed more than 100000 tweets--------------\n",
      "--------------Processing file New_Canada_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processed more than 100000 tweets--------------\n",
      "--------------Processing file New_ICRC_library.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file New_IFRC_Europe.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file New_Ireland_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file New_Nairobi_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file New_nepal_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file New_philredcross.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file New_RedCross.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processed more than 100000 tweets--------------\n",
      "--------------Processing file New_RedCrossLebanon.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file philredcross.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file RedCross.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file RedCrossLebanon.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n"
     ]
    }
   ],
   "source": [
    "all_columns=['conversation_id', 'id','author_id','text','in_reply_to_user_id']\n",
    "for file in os.listdir(\"conversations\"):\n",
    "    if file.split(\".\")[1]==\"json\":\n",
    "        print(\"--------------Processing file {0}--------------\".format(file))\n",
    "        df=pd.read_json(os.path.join(\"conversations\",file),lines=True)\n",
    "        all_tweets=[]\n",
    "        i=100\n",
    "        for line in df['data']:\n",
    "            for tweet in line:\n",
    "                tweet_info=[]\n",
    "\n",
    "                for column in all_columns:\n",
    "                    tweet_info.append(tweet[column])\n",
    "\n",
    "                if tweet.get('referenced_tweets','')!='':\n",
    "                    if tweet['referenced_tweets'][0]['type']=='replied_to':\n",
    "                        tweet_info.append(tweet['referenced_tweets'][0]['id'])\n",
    "                    else:\n",
    "                        tweet_info.append('')\n",
    "                else:\n",
    "                    tweet_info.append('')\n",
    "                \n",
    "                if tweet.get('attachments','')!='':\n",
    "                    tweet_info.append('1')\n",
    "                else:\n",
    "                    tweet_info.append('0')\n",
    "\n",
    "                all_tweets.append(tweet_info)\n",
    "\n",
    "            if len(all_tweets)>i:\n",
    "                print(\"--------------Processed more than {0} tweets--------------\".format(i))\n",
    "                i=i*10\n",
    "\n",
    "        for line in df['includes']:\n",
    "            for tweet in line['tweets']:\n",
    "                tweet_info=[]\n",
    "\n",
    "                for column in all_columns:\n",
    "                    tweet_info.append(tweet.get(column,''))\n",
    "\n",
    "                if tweet.get('referenced_tweets','')!='':\n",
    "                    if tweet['referenced_tweets'][0]['type']=='replied_to':\n",
    "                        tweet_info.append(tweet['referenced_tweets'][0]['id'])\n",
    "                    else:\n",
    "                        tweet_info.append('')\n",
    "                else:\n",
    "                    tweet_info.append('')\n",
    "                \n",
    "                if tweet.get('attachments','')!='':\n",
    "                    tweet_info.append('1')\n",
    "                else:\n",
    "                    tweet_info.append('0')\n",
    "                \n",
    "                all_tweets.append(tweet_info)\n",
    "\n",
    "            if len(all_tweets)>i:\n",
    "                print(\"--------------Processed more than {0} tweets--------------\".format(i))\n",
    "                i=i*10\n",
    "\n",
    "        df_out = pd.DataFrame(all_tweets,columns=all_columns+['replied_to', 'attachment'])\n",
    "        df_out = df_out.drop_duplicates().copy()\n",
    "        df_out.to_csv(\"Parsed_conversations/\"+file.split(\".\")[0]+\".csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing users info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"users.jsonl\",lines=True)\n",
    "all_users=[]\n",
    "public_columns=['followers_count', 'following_count','tweet_count']\n",
    "additional_columns=['id','name', 'username', 'description','created_at']\n",
    "for line in df['data']:\n",
    "    for user in line:\n",
    "\n",
    "        user_info=[]        \n",
    "        for column in public_columns:\n",
    "            try:\n",
    "                user_info.append(user['public_metrics'][column])\n",
    "            except:\n",
    "                print(user)\n",
    "        for column in additional_columns:\n",
    "            user_info.append(user[column])\n",
    "\n",
    "        all_users.append(user_info)\n",
    "\n",
    "df_out = pd.DataFrame(all_users,columns=public_columns+additional_columns)\n",
    "df_out = df_out.drop_duplicates().copy()\n",
    "df_out.to_csv(\"labeled/users.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concating old and new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivan_dict={\"nepal_ids\":\"NepalRedCross\",\"Nairobi_ids\":\"ICRC_Nairobi\",\"Canada_ids\":\"redcrosscanada\",\"Britain_ids\":\"BritishRedCross\",\"Australia_ids\":\"RedCrossAU\",\"Ireland_ids\":\"irishredcross\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGO_list = [\"Australia_ids\", \"Britain_ids\", \"Canada_ids\", \"Ireland_ids\", \"Nairobi_ids\", \"nepal_ids\", \"RedCrossLebanon\", \"RedCross\", \"philredcross\", \"IFRC_Europe\", \"ICRC_library\"]\n",
    "files=os.listdir(\"parsed_conversations\")\n",
    "for ngo in NGO_list:\n",
    "    group=[file for file in files if ngo in file]\n",
    "    if 'RedCross.csv' in group:\n",
    "        group=[element for element in group if 'Lebanon' not in element]\n",
    "    for ind,element in enumerate(group):\n",
    "        if ind==0:\n",
    "            df=pd.read_csv(\"parsed_conversations/\"+element, dtype='str')\n",
    "        else:\n",
    "            df1=pd.read_csv(\"parsed_conversations/\"+element, dtype='str')\n",
    "            df=pd.concat([df,df1]).copy()\n",
    "    df.to_csv(\"parsed_conversations/\"+ivan_dict.get(ngo,ngo)+\"_all.csv\",index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to store NGOs ids\n",
    "ICRC_dict={\"RedCrossAU\":\"138418895\",\n",
    "        \"BritishRedCross\":\"7400692\",\n",
    "        \"philredcross\":\"32391821\",\n",
    "        \"redcrosscanada\":\"16434613\",\n",
    "        \"ICRC_Nairobi\":\"1151791608494473219\",\n",
    "        \"NepalRedCross\":\"566269074\",\n",
    "        \"RedCrossLebanon\":\"2548704956\",\n",
    "        \"ICRC_library\":\"2548704956\",\n",
    "        \"RedCross\":\"6519522\",\n",
    "        \"irishredcross\":\"25411906\",\n",
    "        \"IFRC_Europe\":\"937963107456036865\"}\n",
    "\n",
    "AdditionalNGO_dict = {\"Ch_JesusChrist\":\"10047382\",\n",
    "        \"ymca\":\"309679834\",\n",
    "        \"SalvationArmyUS\":\"16729099\",\n",
    "        \"boyscouts\":\"20685982\",\n",
    "        \"girlscouts\":\"103018203\",\n",
    "        \"Habitat_org\":\"33898911\",\n",
    "        \"SavetheChildren\":\"14055301\",\n",
    "        \"WorldVision\":\"14086764\",\n",
    "        \"RESCUEorg\":\"22053725\",\n",
    "        \"CatholicRelief\":\"14496886\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_labeling(org_ids):\n",
    "\n",
    "    #DataFrame to store all labeled replies related to our task\n",
    "    all_replies = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir('parsed_conversations'):\n",
    "        if file.split('.')[1]=='csv': #set '_all. for ICRC'\n",
    "            NGO_name = file.split('.')[0] #set '_all.' for ICRC\n",
    "            NGO_id = org_ids[NGO_name]\n",
    "\n",
    "            #added terminator to fix tokenizer error for some files\n",
    "            df = pd.read_csv(\"parsed_conversations/{0}\".format(file), lineterminator='\\n', dtype='str')\n",
    "\n",
    "            #get the tweets on which users got replies\n",
    "            ngo_replied_to_ids = df[(df[\"author_id\"] == NGO_id) & (df[\"in_reply_to_user_id\"] != NGO_id)][\"replied_to\"].dropna()\n",
    "            ngo_replied_to = df[df['id'].isin(ngo_replied_to_ids)].dropna()\n",
    "            ngo_replied_to[\"label\"] = 1\n",
    "\n",
    "            #get the tweets on which ngo didn't reply\n",
    "            replies_to_ngo = df[(df[\"in_reply_to_user_id\"]==NGO_id) & (df[\"author_id\"]!=NGO_id)]\n",
    "            replies_to_ngo = replies_to_ngo[~replies_to_ngo['id'].isin(ngo_replied_to_ids)].dropna()\n",
    "            replies_to_ngo[\"label\"] = 0\n",
    "\n",
    "            #concatenate these tweets together\n",
    "            all_ngo_replies = pd.concat([replies_to_ngo, ngo_replied_to]).reset_index(drop=True)\n",
    "            all_ngo_replies[\"relatedNGO\"] = NGO_name\n",
    "            \n",
    "            #add current ngo data to global df\n",
    "            all_replies = pd.concat([all_replies, all_ngo_replies]).reset_index(drop=True)\n",
    "\n",
    "            print(\"processed file: {0}\".format(file))\n",
    "            \n",
    "    all_replies.to_csv(\"labeled/labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label ICRC\n",
    "do_labeling(ICRC_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the data(Additional NGO):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label additional\n",
    "do_labeling(AdditionalNGO_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
