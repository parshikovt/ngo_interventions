{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Processing file Australia_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file Britain_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file Canada_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file ICRC_library.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file IFRC_Europe.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file Ireland_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file Nairobi_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file nepal_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processing file New_Australia_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file New_Britain_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processed more than 100000 tweets--------------\n",
      "--------------Processing file New_Canada_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processed more than 100000 tweets--------------\n",
      "--------------Processing file New_ICRC_library.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file New_IFRC_Europe.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file New_Ireland_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file New_Nairobi_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file New_nepal_ids.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processing file New_philredcross.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file New_RedCross.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processed more than 100000 tweets--------------\n",
      "--------------Processing file New_RedCrossLebanon.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file philredcross.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file RedCross.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n",
      "--------------Processed more than 10000 tweets--------------\n",
      "--------------Processing file RedCrossLebanon.json--------------\n",
      "--------------Processed more than 100 tweets--------------\n",
      "--------------Processed more than 1000 tweets--------------\n"
     ]
    }
   ],
   "source": [
    "all_columns=['conversation_id', 'id','author_id','text','in_reply_to_user_id']\n",
    "for file in os.listdir(\"conversations\"):\n",
    "    if file.split(\".\")[1]==\"json\":\n",
    "        print(\"--------------Processing file {0}--------------\".format(file))\n",
    "        df=pd.read_json(os.path.join(\"conversations\",file),lines=True)\n",
    "        all_tweets=[]\n",
    "        i=100\n",
    "        for line in df['data']:\n",
    "            for tweet in line:\n",
    "                tweet_info=[]\n",
    "                for column in all_columns:\n",
    "                    tweet_info.append(tweet[column])\n",
    "                try:\n",
    "                    if tweet['referenced_tweets'][0]['type']=='replied_to':\n",
    "                        tweet_info.append(tweet['referenced_tweets'][0]['id'])\n",
    "                    else:\n",
    "                        tweet_info.append('')\n",
    "                except:\n",
    "                    print(tweet['referenced_tweets'][0])\n",
    "                    tweet_info.append('')\n",
    "                all_tweets.append(tweet_info)\n",
    "            if len(all_tweets)>i:\n",
    "                print(\"--------------Processed more than {0} tweets--------------\".format(i))\n",
    "                i=i*10\n",
    "\n",
    "        for line in df['includes']:\n",
    "            for tweet in line['tweets']:\n",
    "                tweet_info=[]\n",
    "                for column in all_columns:\n",
    "                    tweet_info.append(tweet.get(column,''))\n",
    "                if tweet.get('referenced_tweets','')!='':\n",
    "                    if tweet['referenced_tweets'][0]['type']=='replied_to':\n",
    "                        tweet_info.append(tweet['referenced_tweets'][0]['id'])\n",
    "                    else:\n",
    "                        tweet_info.append('')\n",
    "                else:\n",
    "                    tweet_info.append('')\n",
    "                all_tweets.append(tweet_info)\n",
    "            if len(all_tweets)>i:\n",
    "                print(\"--------------Processed more than {0} tweets--------------\".format(i))\n",
    "                i=i*10\n",
    "        df_out = pd.DataFrame(all_tweets,columns=all_columns+['replied_to'])\n",
    "        df_out = df_out.drop_duplicates().copy()\n",
    "        df_out.to_csv(\"Parsed_conversations/\"+file.split(\".\")[0]+\".csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concating old and new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivan_dict={\"nepal_ids\":\"NepalRedCross\",\"Nairobi_ids\":\"ICRC_Nairobi\",\"Canada_ids\":\"redcrosscanada\",\"Britain_ids\":\"BritishRedCross\",\"Australia_ids\":\"RedCrossAU\",\"Ireland_ids\":\"irishredcross\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGO_list = [\"Australia_ids\", \"Britain_ids\", \"Canada_ids\", \"Ireland_ids\", \"Nairobi_ids\", \"nepal_ids\", \"RedCrossLebanon\", \"RedCross\", \"philredcross\", \"IFRC_Europe\", \"ICRC_library\"]\n",
    "files=os.listdir(\"parsed_conversations\")\n",
    "for ngo in NGO_list:\n",
    "    group=[file for file in files if ngo in file]\n",
    "    if 'RedCross.csv' in group:\n",
    "        group=[element for element in group if 'Lebanon' not in element]\n",
    "    for ind,element in enumerate(group):\n",
    "        if ind==0:\n",
    "            df=pd.read_csv(\"parsed_conversations/\"+element)\n",
    "        else:\n",
    "            df1=pd.read_csv(\"parsed_conversations/\"+element)\n",
    "            df=pd.concat([df,df1]).copy()\n",
    "    df.to_csv(\"parsed_conversations/\"+ivan_dict.get(ngo,ngo)+\"_all.csv\",index=False)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semester_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9215884f317b28951366236cef9f8992a096a5b1fa6c5fa4d454321ff0331c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
